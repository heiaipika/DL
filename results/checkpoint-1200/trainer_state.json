{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 9.324225425720215,
      "learning_rate": 4.962500000000001e-05,
      "loss": 0.7562,
      "step": 10
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 19.736387252807617,
      "learning_rate": 4.9208333333333335e-05,
      "loss": 0.6308,
      "step": 20
    },
    {
      "epoch": 0.025,
      "grad_norm": 13.55260944366455,
      "learning_rate": 4.879166666666667e-05,
      "loss": 0.6602,
      "step": 30
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 19.732555389404297,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 0.4569,
      "step": 40
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 11.7695951461792,
      "learning_rate": 4.795833333333333e-05,
      "loss": 0.4965,
      "step": 50
    },
    {
      "epoch": 0.05,
      "grad_norm": 10.626523971557617,
      "learning_rate": 4.7541666666666666e-05,
      "loss": 0.5251,
      "step": 60
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 16.420419692993164,
      "learning_rate": 4.7125e-05,
      "loss": 0.4146,
      "step": 70
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 6.054170608520508,
      "learning_rate": 4.6708333333333335e-05,
      "loss": 0.3461,
      "step": 80
    },
    {
      "epoch": 0.075,
      "grad_norm": 23.564321517944336,
      "learning_rate": 4.629166666666667e-05,
      "loss": 0.4131,
      "step": 90
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 140.7157440185547,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 0.3064,
      "step": 100
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 7.332809925079346,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.2424,
      "step": 110
    },
    {
      "epoch": 0.1,
      "grad_norm": 18.344085693359375,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.5224,
      "step": 120
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 1.8417422771453857,
      "learning_rate": 4.4625e-05,
      "loss": 0.4057,
      "step": 130
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 46.97586441040039,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.4634,
      "step": 140
    },
    {
      "epoch": 0.125,
      "grad_norm": 6.242224216461182,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.3868,
      "step": 150
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.4157070815563202,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.2527,
      "step": 160
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 3.4641950130462646,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.3069,
      "step": 170
    },
    {
      "epoch": 0.15,
      "grad_norm": 8.421256065368652,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.5134,
      "step": 180
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 10.55270767211914,
      "learning_rate": 4.2125e-05,
      "loss": 0.6538,
      "step": 190
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 42.55424880981445,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.3083,
      "step": 200
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.5155918598175049,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.2663,
      "step": 210
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 11.36816120147705,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.615,
      "step": 220
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 5.203353404998779,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.3361,
      "step": 230
    },
    {
      "epoch": 0.2,
      "grad_norm": 27.18766975402832,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.3589,
      "step": 240
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 6.012739658355713,
      "learning_rate": 3.9625e-05,
      "loss": 0.4258,
      "step": 250
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 9.713712692260742,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.4181,
      "step": 260
    },
    {
      "epoch": 0.225,
      "grad_norm": 5.4857916831970215,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.2915,
      "step": 270
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 10.203445434570312,
      "learning_rate": 3.8375e-05,
      "loss": 0.3532,
      "step": 280
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 21.580890655517578,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.3793,
      "step": 290
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.9196268320083618,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.2594,
      "step": 300
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.4633036851882935,
      "learning_rate": 3.7125e-05,
      "loss": 0.4793,
      "step": 310
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 4.111232280731201,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.2885,
      "step": 320
    },
    {
      "epoch": 0.275,
      "grad_norm": 8.6216402053833,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.3422,
      "step": 330
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 25.80129051208496,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.4498,
      "step": 340
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 3.8594071865081787,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.4906,
      "step": 350
    },
    {
      "epoch": 0.3,
      "grad_norm": 31.888195037841797,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.3124,
      "step": 360
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 8.209714889526367,
      "learning_rate": 3.4625e-05,
      "loss": 0.2153,
      "step": 370
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 32.0982551574707,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.2648,
      "step": 380
    },
    {
      "epoch": 0.325,
      "grad_norm": 5.303349494934082,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.3374,
      "step": 390
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 22.476442337036133,
      "learning_rate": 3.3375e-05,
      "loss": 0.136,
      "step": 400
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.6083981394767761,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.3122,
      "step": 410
    },
    {
      "epoch": 0.35,
      "grad_norm": 16.12623405456543,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.1799,
      "step": 420
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 21.25523567199707,
      "learning_rate": 3.2125e-05,
      "loss": 0.4917,
      "step": 430
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 5.151843070983887,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.6876,
      "step": 440
    },
    {
      "epoch": 0.375,
      "grad_norm": 12.06717586517334,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.4375,
      "step": 450
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.9442533254623413,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.3263,
      "step": 460
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.2738977670669556,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.3763,
      "step": 470
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.26196542382240295,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.3236,
      "step": 480
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.34845301508903503,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.4403,
      "step": 490
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 5.0214409828186035,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.3414,
      "step": 500
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.3672044277191162,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.3942,
      "step": 510
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.807505190372467,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.3129,
      "step": 520
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.7256729006767273,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.5616,
      "step": 530
    },
    {
      "epoch": 0.45,
      "grad_norm": 21.146120071411133,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.5919,
      "step": 540
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 5.099173069000244,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.3911,
      "step": 550
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.45354950428009033,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.3713,
      "step": 560
    },
    {
      "epoch": 0.475,
      "grad_norm": 26.78803062438965,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.1986,
      "step": 570
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 5.2665019035339355,
      "learning_rate": 2.5875e-05,
      "loss": 0.5239,
      "step": 580
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 16.85371208190918,
      "learning_rate": 2.5458333333333333e-05,
      "loss": 0.5938,
      "step": 590
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7088008522987366,
      "learning_rate": 2.5041666666666668e-05,
      "loss": 0.2466,
      "step": 600
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.0145645141601562,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.2573,
      "step": 610
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.4606645405292511,
      "learning_rate": 2.4208333333333337e-05,
      "loss": 0.3174,
      "step": 620
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.5716087818145752,
      "learning_rate": 2.3791666666666668e-05,
      "loss": 0.3177,
      "step": 630
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.22234828770160675,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.1538,
      "step": 640
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 4.73409366607666,
      "learning_rate": 2.2958333333333333e-05,
      "loss": 0.2977,
      "step": 650
    },
    {
      "epoch": 0.55,
      "grad_norm": 11.204654693603516,
      "learning_rate": 2.2541666666666668e-05,
      "loss": 0.336,
      "step": 660
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 1.0272632837295532,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 0.3127,
      "step": 670
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 15.932574272155762,
      "learning_rate": 2.1708333333333334e-05,
      "loss": 0.1754,
      "step": 680
    },
    {
      "epoch": 0.575,
      "grad_norm": 2.066206932067871,
      "learning_rate": 2.1291666666666668e-05,
      "loss": 0.5034,
      "step": 690
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 1.5098766088485718,
      "learning_rate": 2.0875e-05,
      "loss": 0.3027,
      "step": 700
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 13.793088912963867,
      "learning_rate": 2.0458333333333334e-05,
      "loss": 0.2953,
      "step": 710
    },
    {
      "epoch": 0.6,
      "grad_norm": 12.040916442871094,
      "learning_rate": 2.0041666666666668e-05,
      "loss": 0.6389,
      "step": 720
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.8141654133796692,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.2941,
      "step": 730
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 27.061416625976562,
      "learning_rate": 1.9208333333333334e-05,
      "loss": 0.3123,
      "step": 740
    },
    {
      "epoch": 0.625,
      "grad_norm": 9.792169570922852,
      "learning_rate": 1.8791666666666668e-05,
      "loss": 0.3066,
      "step": 750
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 36.258094787597656,
      "learning_rate": 1.8375e-05,
      "loss": 0.3914,
      "step": 760
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 35.93669891357422,
      "learning_rate": 1.7958333333333334e-05,
      "loss": 0.304,
      "step": 770
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.8386331796646118,
      "learning_rate": 1.754166666666667e-05,
      "loss": 0.3195,
      "step": 780
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 26.679861068725586,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.4172,
      "step": 790
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 13.489230155944824,
      "learning_rate": 1.6708333333333334e-05,
      "loss": 0.421,
      "step": 800
    },
    {
      "epoch": 0.675,
      "grad_norm": 11.613401412963867,
      "learning_rate": 1.6291666666666665e-05,
      "loss": 0.2386,
      "step": 810
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 39.90391159057617,
      "learning_rate": 1.5875e-05,
      "loss": 0.3003,
      "step": 820
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 7.3435492515563965,
      "learning_rate": 1.5458333333333334e-05,
      "loss": 0.2477,
      "step": 830
    },
    {
      "epoch": 0.7,
      "grad_norm": 6.081480026245117,
      "learning_rate": 1.5041666666666669e-05,
      "loss": 0.3114,
      "step": 840
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 49.40216827392578,
      "learning_rate": 1.4625e-05,
      "loss": 0.3768,
      "step": 850
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 7.07928466796875,
      "learning_rate": 1.4208333333333334e-05,
      "loss": 0.2769,
      "step": 860
    },
    {
      "epoch": 0.725,
      "grad_norm": 5.263875484466553,
      "learning_rate": 1.3791666666666667e-05,
      "loss": 0.4118,
      "step": 870
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 6.305405139923096,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 0.5447,
      "step": 880
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.6146869659423828,
      "learning_rate": 1.2958333333333333e-05,
      "loss": 0.1721,
      "step": 890
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.6554181575775146,
      "learning_rate": 1.2541666666666669e-05,
      "loss": 0.2577,
      "step": 900
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.2889532446861267,
      "learning_rate": 1.2125e-05,
      "loss": 0.2936,
      "step": 910
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 5.5888495445251465,
      "learning_rate": 1.1708333333333334e-05,
      "loss": 0.223,
      "step": 920
    },
    {
      "epoch": 0.775,
      "grad_norm": 5.299344062805176,
      "learning_rate": 1.1291666666666667e-05,
      "loss": 0.3388,
      "step": 930
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 5.144724369049072,
      "learning_rate": 1.0875e-05,
      "loss": 0.4114,
      "step": 940
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 9.963360786437988,
      "learning_rate": 1.0458333333333335e-05,
      "loss": 0.3174,
      "step": 950
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5764057636260986,
      "learning_rate": 1.0041666666666667e-05,
      "loss": 0.1458,
      "step": 960
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 5.585819721221924,
      "learning_rate": 9.625e-06,
      "loss": 0.4079,
      "step": 970
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 29.297361373901367,
      "learning_rate": 9.208333333333335e-06,
      "loss": 0.414,
      "step": 980
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.38414496183395386,
      "learning_rate": 8.791666666666667e-06,
      "loss": 0.233,
      "step": 990
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 62.479339599609375,
      "learning_rate": 8.375e-06,
      "loss": 0.417,
      "step": 1000
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 5.4055914878845215,
      "learning_rate": 7.958333333333335e-06,
      "loss": 0.3156,
      "step": 1010
    },
    {
      "epoch": 0.85,
      "grad_norm": 6.698360919952393,
      "learning_rate": 7.541666666666668e-06,
      "loss": 0.441,
      "step": 1020
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 1.4131048917770386,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 0.1469,
      "step": 1030
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 6.352024555206299,
      "learning_rate": 6.708333333333333e-06,
      "loss": 0.1922,
      "step": 1040
    },
    {
      "epoch": 0.875,
      "grad_norm": 15.864017486572266,
      "learning_rate": 6.291666666666667e-06,
      "loss": 0.5073,
      "step": 1050
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.7444868683815002,
      "learning_rate": 5.875e-06,
      "loss": 0.4308,
      "step": 1060
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 6.115273952484131,
      "learning_rate": 5.458333333333333e-06,
      "loss": 0.291,
      "step": 1070
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.4982070922851562,
      "learning_rate": 5.041666666666667e-06,
      "loss": 0.2498,
      "step": 1080
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 1.0724599361419678,
      "learning_rate": 4.625e-06,
      "loss": 0.4066,
      "step": 1090
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 97.56006622314453,
      "learning_rate": 4.208333333333333e-06,
      "loss": 0.3356,
      "step": 1100
    },
    {
      "epoch": 0.925,
      "grad_norm": 8.000947952270508,
      "learning_rate": 3.791666666666667e-06,
      "loss": 0.4617,
      "step": 1110
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 26.866060256958008,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.3811,
      "step": 1120
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 17.323209762573242,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 0.3989,
      "step": 1130
    },
    {
      "epoch": 0.95,
      "grad_norm": 13.664201736450195,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 0.1475,
      "step": 1140
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.5097546577453613,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.1911,
      "step": 1150
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 11.832951545715332,
      "learning_rate": 1.7083333333333332e-06,
      "loss": 0.4012,
      "step": 1160
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.9982109069824219,
      "learning_rate": 1.2916666666666667e-06,
      "loss": 0.3529,
      "step": 1170
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 11.957881927490234,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.1241,
      "step": 1180
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.8151993155479431,
      "learning_rate": 4.583333333333334e-07,
      "loss": 0.2285,
      "step": 1190
    },
    {
      "epoch": 1.0,
      "grad_norm": 25.177114486694336,
      "learning_rate": 4.166666666666667e-08,
      "loss": 0.3928,
      "step": 1200
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3153775930404663,
      "eval_runtime": 10.2256,
      "eval_samples_per_second": 117.353,
      "eval_steps_per_second": 14.669,
      "step": 1200
    }
  ],
  "logging_steps": 10,
  "max_steps": 1200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 631472202547200.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
